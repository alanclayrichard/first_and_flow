"""
NFL Data Loader for Flow Matching Model

Handles loading and preprocessing of NFL team performance data
generated by the R data pipeline.
"""

import pandas as pd
import numpy as np
from typing import Tuple, Dict, List, Optional
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class NFLDataLoader:
    """
    Loads and preprocesses NFL team performance data for flow matching models.
    
    The data loader handles:
    - Loading CSV data from the data generation pipeline
    - Feature engineering and normalization
    - Train/validation/test splitting
    - Sequence preparation for flow matching
    """
    
    def __init__(
        self,
        data_path: str = "../../data/processed/nfl_flow_training_data.csv",
        normalize_features: bool = True,
        sequence_length: int = 4  # 4-week sequences
    ):
        """
        Initialize the NFL data loader.
        
        Args:
            data_path: Path to the processed NFL CSV data
            normalize_features: Whether to normalize feature values
            sequence_length: Number of weeks in each sequence
        """
        self.data_path = Path(data_path)
        self.normalize_features = normalize_features
        self.sequence_length = sequence_length
        self.data = None
        self.feature_stats = {}
        
    def load_data(self) -> pd.DataFrame:
        """Load the NFL data from CSV."""
        if not self.data_path.exists():
            raise FileNotFoundError(
                f"Data file not found: {self.data_path}\n"
                "Please run the data generation pipeline first:\n"
                "cd data && Rscript generate_data.R"
            )
        
        logger.info(f"Loading NFL data from {self.data_path}")
        self.data = pd.read_csv(self.data_path)
        
        logger.info(f"Loaded {len(self.data)} team-week observations")
        logger.info(f"Seasons: {self.data['season'].min()} to {self.data['season'].max()}")
        logger.info(f"Teams: {self.data['team'].nunique()} unique teams")
        
        return self.data
    
    def preprocess_features(self) -> pd.DataFrame:
        """
        Preprocess and engineer features for flow matching.
        
        Returns:
            DataFrame with processed features
        """
        if self.data is None:
            self.load_data()
        
        df = self.data.copy()
        
        # Feature engineering
        logger.info("Engineering features for flow matching...")
        
        # Add derived features
        df['epa_success_ratio'] = df['off_epa_per_play'] / (df['success_rate'] + 1e-8)
        df['efficiency_score'] = df['off_epa_per_play'] * df['success_rate']
        df['passing_efficiency'] = df['avg_air_yards'] * df['pass_rate']
        df['plays_per_game'] = df['total_plays'] / 1.0  # Normalize if needed
        
        # Select features for flow matching
        feature_cols = [
            'off_epa_per_play',
            'success_rate', 
            'pass_rate',
            'avg_air_yards',
            'efficiency_score',
            'passing_efficiency',
            'total_plays'
        ]
        
        # Handle missing values
        df[feature_cols] = df[feature_cols].fillna(df[feature_cols].median())
        
        # Normalize features if requested
        if self.normalize_features:
            df, self.feature_stats = self._normalize_features(df, feature_cols)
        
        logger.info(f"Preprocessed {len(feature_cols)} features")
        return df
    
    def _normalize_features(
        self, 
        df: pd.DataFrame, 
        feature_cols: List[str]
    ) -> Tuple[pd.DataFrame, Dict]:
        """Normalize features to [0, 1] range."""
        stats = {}
        df_norm = df.copy()
        
        for col in feature_cols:
            min_val = df[col].min()
            max_val = df[col].max()
            stats[col] = {'min': min_val, 'max': max_val}
            
            # Min-max normalization
            df_norm[col] = (df[col] - min_val) / (max_val - min_val + 1e-8)
        
        logger.info("Features normalized to [0, 1] range")
        return df_norm, stats
    
    def create_sequences(
        self, 
        df: pd.DataFrame
    ) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """
        Create sequences of team performance for flow matching.
        
        Args:
            df: Preprocessed DataFrame
            
        Returns:
            Tuple of (sequences, team_labels, team_names)
        """
        logger.info(f"Creating {self.sequence_length}-week sequences...")
        
        # Use the same feature columns that were defined in preprocess_features
        feature_cols = [
            'off_epa_per_play',
            'success_rate', 
            'pass_rate',
            'avg_air_yards',
            'efficiency_score',
            'passing_efficiency',
            'total_plays'
        ]
        
        # Verify all features are present in the DataFrame
        missing_features = [feat for feat in feature_cols if feat not in df.columns]
        if missing_features:
            logger.warning(f"Missing features: {missing_features}")
            feature_cols = [feat for feat in feature_cols if feat in df.columns]
        
        logger.info(f"Using features: {feature_cols}")
        
        sequences = []
        team_labels = []
        team_names = []
        
        # Group by team and season
        for (team, season), group in df.groupby(['team', 'season']):
            # Sort by week
            group_sorted = group.sort_values('week')
            
            # Create sequences of consecutive weeks
            for i in range(len(group_sorted) - self.sequence_length + 1):
                sequence = group_sorted.iloc[i:i+self.sequence_length][feature_cols].values
                sequences.append(sequence)
                team_labels.append(team)
                team_names.append(f"{team}_{season}_week{group_sorted.iloc[i]['week']}")
        
        sequences = np.array(sequences)
        logger.info(f"Created {len(sequences)} sequences of shape {sequences.shape}")
        
        return sequences, np.array(team_labels), team_names
    
    def train_test_split(
        self,
        test_seasons: Optional[List[int]] = None,
        validation_split: float = 0.2
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        Split data into train/validation/test sets.
        
        Args:
            test_seasons: Seasons to use for testing (default: latest season)
            validation_split: Fraction of training data for validation
            
        Returns:
            Tuple of (train_df, val_df, test_df)
        """
        if self.data is None:
            raise ValueError("Data not loaded. Call load_data() first.")
        
        df = self.data.copy()
        
        # Default to using latest season for testing
        if test_seasons is None:
            test_seasons = [df['season'].max()]
        
        # Split by season for temporal consistency
        test_df = df[df['season'].isin(test_seasons)]
        train_val_df = df[~df['season'].isin(test_seasons)]
        
        # Random split for train/validation within remaining seasons
        np.random.seed(42)
        teams = train_val_df['team'].unique()
        val_teams = np.random.choice(
            teams, 
            size=int(len(teams) * validation_split), 
            replace=False
        )
        
        val_df = train_val_df[train_val_df['team'].isin(val_teams)]
        train_df = train_val_df[~train_val_df['team'].isin(val_teams)]
        
        logger.info(f"Data split: {len(train_df)} train, {len(val_df)} val, {len(test_df)} test")
        
        return train_df, val_df, test_df
    
    def get_team_trajectories(self, team: str, season: int) -> np.ndarray:
        """
        Get the performance trajectory for a specific team and season.
        
        Args:
            team: Team abbreviation (e.g., "BUF")
            season: Season year
            
        Returns:
            Array of weekly performance metrics
        """
        if self.data is None:
            raise ValueError("Data not loaded. Call load_data() first.")
        
        team_data = self.data[
            (self.data['team'] == team) & 
            (self.data['season'] == season)
        ].sort_values('week')
        
        # Use the same feature columns as in preprocess_features and create_sequences
        feature_cols = [
            'off_epa_per_play',
            'success_rate', 
            'pass_rate',
            'avg_air_yards',
            'efficiency_score',
            'passing_efficiency',
            'total_plays'
        ]
        
        # Check which features are available in the data
        available_features = [feat for feat in feature_cols if feat in team_data.columns]
        if len(available_features) != len(feature_cols):
            logger.warning(f"Some features missing. Available: {available_features}")
            feature_cols = available_features
        
        return team_data[feature_cols].values


if __name__ == "__main__":
    # Example usage
    loader = NFLDataLoader()
    
    # Load and preprocess data
    data = loader.load_data()
    processed_data = loader.preprocess_features()
    
    # Create sequences
    sequences, labels, names = loader.create_sequences(processed_data)
    
    print(f"Generated {len(sequences)} sequences for flow matching")
    print(f"Sequence shape: {sequences.shape}")
    print(f"Feature dimensions: {sequences.shape[-1]}")
